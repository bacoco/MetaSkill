#!/usr/bin/env python3
"""
Synapse Pattern Detector
Sophisticated analysis of Cortex files to detect patterns and recommend skills.

Architecture:
- CortexDataReader: Reads and parses Cortex files
- PatternDetector: Detects sophisticated patterns
- SkillRecommender: Recommends skills based on patterns
- ReportGenerator: Generates detailed reports
- ConfigManager: Manages configuration

Generated by Synapse - Pattern Detection Engine
"""

import json
import os
import re
import subprocess
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
import argparse
import logging


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class SessionData:
    """Represents a single Cortex session"""
    timestamp: str
    agent: str
    repository: str
    total_files: int
    modified_files: int
    added_files: int
    deleted_files: int
    untracked_files: int
    file_categories: Dict[str, List[str]]
    recent_commits: List[Dict[str, str]]
    problems_solved: List[str]
    git_status: str
    branch: str
    changed_files: List[str]


@dataclass
class PatternInfo:
    """Information about a detected pattern"""
    pattern_type: str
    description: str
    frequency: int
    impact_score: float
    trend_score: float
    urgency_score: float
    examples: List[str]
    metadata: Dict = field(default_factory=dict)


@dataclass
class SkillRecommendation:
    """Recommendation for a new skill"""
    skill_name: str
    skill_type: str
    description: str
    reason: str
    priority_score: float
    frequency_score: float
    impact_score: float
    trend_score: float
    urgency_score: float
    roi_score: float
    supporting_patterns: List[str]
    example_use_cases: List[str]


@dataclass
class TrendAnalysis:
    """Trend analysis over time"""
    increasing_patterns: List[str]
    decreasing_patterns: List[str]
    stable_patterns: List[str]
    emerging_patterns: List[str]
    session_frequency: float
    avg_files_per_session: float
    most_active_periods: List[str]


# ============================================================================
# CONFIG MANAGER
# ============================================================================

class ConfigManager:
    """Manages configuration and thresholds"""

    DEFAULT_CONFIG = {
        "thresholds": {
            "pattern_frequency_min": 3,
            "pattern_frequency_high": 6,
            "impact_score_threshold": 0.5,
            "trend_score_threshold": 0.3,
            "urgency_score_threshold": 0.6,
            "roi_score_threshold": 0.7,
            "recommendation_min_score": 0.5
        },
        "analysis": {
            "max_sessions_to_analyze": 50,
            "file_correlation_threshold": 2,
            "problem_recurrence_threshold": 2,
            "temporal_pattern_window_days": 7,
            "enable_ml_clustering": False,
            "enable_nlp_analysis": False
        },
        "scoring": {
            "frequency_weight": 0.25,
            "impact_weight": 0.25,
            "trend_weight": 0.15,
            "urgency_weight": 0.20,
            "roi_weight": 0.15
        },
        "output": {
            "report_format": "both",  # "json", "text", "both"
            "include_examples": True,
            "include_visualizations": False,
            "verbose": True
        }
    }

    def __init__(self, config_path: Optional[str] = None):
        self.config = self.load_config(config_path)
        self.logger = logging.getLogger(__name__)

    def load_config(self, config_path: Optional[str]) -> Dict:
        """Load configuration from file or use defaults"""
        config = self.DEFAULT_CONFIG.copy()

        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    user_config = json.load(f)
                    # Deep merge
                    for key, value in user_config.items():
                        if isinstance(value, dict) and key in config:
                            config[key].update(value)
                        else:
                            config[key] = value
                self.logger.info(f"Loaded configuration from {config_path}")
            except Exception as e:
                self.logger.warning(f"Failed to load config from {config_path}: {e}")

        return config

    def get(self, *keys, default=None):
        """Get nested config value"""
        value = self.config
        for key in keys:
            if isinstance(value, dict):
                value = value.get(key)
            else:
                return default
            if value is None:
                return default
        return value

    def validate(self) -> bool:
        """Validate configuration"""
        # Check required keys
        required_keys = ["thresholds", "analysis", "scoring", "output"]
        for key in required_keys:
            if key not in self.config:
                self.logger.error(f"Missing required config key: {key}")
                return False

        # Validate scoring weights sum to 1.0
        weights = self.config["scoring"]
        total_weight = sum([
            weights["frequency_weight"],
            weights["impact_weight"],
            weights["trend_weight"],
            weights["urgency_weight"],
            weights["roi_weight"]
        ])

        if abs(total_weight - 1.0) > 0.01:
            self.logger.warning(f"Scoring weights sum to {total_weight}, not 1.0")

        return True


# ============================================================================
# Cortex DATA READER
# ============================================================================

class CortexDataReader:
    """Reads and parses Cortex files"""

    def __init__(self, repo_root: str, config: ConfigManager):
        self.repo_root = Path(repo_root)
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Cortex file paths
        self.agent_log_path = self.repo_root / ".cortex_log.md"
        self.agent_status_path = self.repo_root / ".cortex_status.json"
        self.agent_handoff_path = self.repo_root / ".cortex_handoff.md"

    def read_all_soul_data(self) -> Dict:
        """Read all Cortex files and return structured data"""
        data = {
            "sessions": [],
            "current_status": {},
            "handoff_info": {},
            "git_commits": []
        }

        try:
            # Read session logs
            if self.agent_log_path.exists():
                data["sessions"] = self.parse_agent_log()
            else:
                self.logger.warning(f"Agent log not found: {self.agent_log_path}")

            # Read current status
            if self.agent_status_path.exists():
                data["current_status"] = self.parse_agent_status()
            else:
                self.logger.warning(f"Agent status not found: {self.agent_status_path}")

            # Read handoff info
            if self.agent_handoff_path.exists():
                data["handoff_info"] = self.parse_agent_handoff()
            else:
                self.logger.warning(f"Agent handoff not found: {self.agent_handoff_path}")

            # Read Git commits
            data["git_commits"] = self.read_git_commits()

        except Exception as e:
            self.logger.error(f"Error reading Cortex data: {e}")

        return data

    def parse_agent_log(self) -> List[SessionData]:
        """Parse .cortex_log.md into structured session data"""
        sessions = []

        try:
            with open(self.agent_log_path, 'r') as f:
                content = f.read()

            # Split by session separator
            session_blocks = content.split("=" * 80)

            for block in session_blocks:
                if not block.strip():
                    continue

                session = self._parse_session_block(block)
                if session:
                    sessions.append(session)

            self.logger.info(f"Parsed {len(sessions)} sessions from agent log")

        except Exception as e:
            self.logger.error(f"Error parsing agent log: {e}")

        return sessions

    def _parse_session_block(self, block: str) -> Optional[SessionData]:
        """Parse a single session block from agent log"""
        try:
            # Extract timestamp
            timestamp_match = re.search(r'Session (\d{4}-\d{2}-\d{2}-\d{2}:\d{2})', block)
            timestamp = timestamp_match.group(1) if timestamp_match else ""

            # Extract agent
            agent_match = re.search(r'\*\*Agent\*\*: (.+)', block)
            agent = agent_match.group(1) if agent_match else ""

            # Extract repository
            repo_match = re.search(r'\*\*Repository\*\*: (.+)', block)
            repository = repo_match.group(1) if repo_match else ""

            # Extract file counts
            total_files = self._extract_number(block, r'\*\*Total Files\*\*: (\d+)')
            modified_files = self._extract_number(block, r'\*\*Modified\*\*: (\d+)')
            added_files = self._extract_number(block, r'\*\*Added\*\*: (\d+)')
            deleted_files = self._extract_number(block, r'\*\*Deleted\*\*: (\d+)')
            untracked_files = self._extract_number(block, r'\*\*Untracked\*\*: (\d+)')

            # Extract problems solved
            problems_solved = []
            problem_matches = re.findall(r'\*\*Fix\*\*: (.+)', block)
            problems_solved.extend(problem_matches)

            # Extract git status
            git_status_match = re.search(r'\*\*Git Status\*\*: (.+)', block)
            git_status = git_status_match.group(1) if git_status_match else ""

            # Extract branch
            branch_match = re.search(r'\*\*Branch\*\*: (.+)', block)
            branch = branch_match.group(1) if branch_match else "main"

            # Extract changed files
            changed_files = []
            changed_files_section = re.search(r'### Changed Files\n(.+?)(?:\n\n|\n##|$)', block, re.DOTALL)
            if changed_files_section:
                file_matches = re.findall(r'(?:Modified|Deleted|Added|Untracked): (.+?)(?:\n|,|\(and)', changed_files_section.group(1))
                changed_files.extend(file_matches)

            # Extract recent commits
            recent_commits = []
            commit_matches = re.findall(r'\*\*([a-f0-9]+)\*\*: (.+?) \((\d{4}-\d{2}-\d{2})\)', block)
            for commit_hash, message, date in commit_matches:
                recent_commits.append({
                    "hash": commit_hash,
                    "message": message,
                    "date": date
                })

            return SessionData(
                timestamp=timestamp,
                agent=agent,
                repository=repository,
                total_files=total_files,
                modified_files=modified_files,
                added_files=added_files,
                deleted_files=deleted_files,
                untracked_files=untracked_files,
                file_categories={},
                recent_commits=recent_commits,
                problems_solved=problems_solved,
                git_status=git_status,
                branch=branch,
                changed_files=changed_files
            )

        except Exception as e:
            self.logger.warning(f"Error parsing session block: {e}")
            return None

    def _extract_number(self, text: str, pattern: str) -> int:
        """Extract number from text using regex pattern"""
        match = re.search(pattern, text)
        return int(match.group(1)) if match else 0

    def parse_agent_status(self) -> Dict:
        """Parse .cortex_status.json"""
        try:
            with open(self.agent_status_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"Error parsing agent status: {e}")
            return {}

    def parse_agent_handoff(self) -> Dict:
        """Parse .cortex_handoff.md"""
        handoff_info = {}

        try:
            with open(self.agent_handoff_path, 'r') as f:
                content = f.read()

            # Extract project name
            project_match = re.search(r'\*\*Project\*\*: (.+)', content)
            handoff_info["project"] = project_match.group(1) if project_match else ""

            # Extract status
            status_match = re.search(r'\*\*Status\*\*: (.+)', content)
            handoff_info["status"] = status_match.group(1) if status_match else ""

            # Extract next steps
            next_steps = []
            steps_section = re.search(r'## ðŸ“‹ Next Steps.*?\n(.*?)(?:\n##|$)', content, re.DOTALL)
            if steps_section:
                step_matches = re.findall(r'\d+\.\s+(.+)', steps_section.group(1))
                next_steps.extend(step_matches)
            handoff_info["next_steps"] = next_steps

            # Extract recent work
            recent_work_match = re.search(r'- Latest: (.+)', content)
            handoff_info["recent_work"] = recent_work_match.group(1) if recent_work_match else ""

        except Exception as e:
            self.logger.error(f"Error parsing agent handoff: {e}")

        return handoff_info

    def read_git_commits(self, limit: int = 50) -> List[Dict]:
        """Read recent Git commits"""
        commits = []

        try:
            result = subprocess.run(
                ["git", "log", f"-{limit}", "--pretty=format:%H|%an|%ae|%ad|%s"],
                cwd=self.repo_root,
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if not line:
                        continue
                    parts = line.split('|')
                    if len(parts) == 5:
                        commits.append({
                            "hash": parts[0],
                            "author": parts[1],
                            "email": parts[2],
                            "date": parts[3],
                            "message": parts[4]
                        })

            self.logger.info(f"Read {len(commits)} Git commits")

        except Exception as e:
            self.logger.warning(f"Error reading Git commits: {e}")

        return commits


# ============================================================================
# PATTERN DETECTOR
# ============================================================================

class PatternDetector:
    """Detects sophisticated patterns in Cortex data"""

    def __init__(self, config: ConfigManager):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def analyze_all_patterns(self, soul_data: Dict) -> Dict[str, PatternInfo]:
        """Analyze all patterns in Cortex data"""
        patterns = {}

        sessions = soul_data.get("sessions", [])
        commits = soul_data.get("git_commits", [])

        if not sessions:
            self.logger.warning("No sessions to analyze")
            return patterns

        # Temporal patterns
        patterns.update(self.analyze_temporal_patterns(sessions))

        # File correlation patterns
        patterns.update(self.analyze_file_correlations(sessions))

        # Problem recurrence patterns
        patterns.update(self.analyze_problem_recurrence(sessions))

        # Commit patterns
        patterns.update(self.analyze_commit_patterns(commits))

        # Agent collaboration patterns
        patterns.update(self.analyze_agent_collaboration(sessions))

        # Skill gap patterns
        patterns.update(self.analyze_skill_gaps(sessions, commits))

        self.logger.info(f"Detected {len(patterns)} patterns")

        return patterns

    def analyze_temporal_patterns(self, sessions: List[SessionData]) -> Dict[str, PatternInfo]:
        """Analyze temporal patterns in sessions"""
        patterns = {}

        if not sessions:
            return patterns

        # Extract timestamps
        timestamps = []
        for session in sessions:
            try:
                # Parse timestamp format: 2025-10-25-12:26
                dt = datetime.strptime(session.timestamp, "%Y-%m-%d-%H:%M")
                timestamps.append(dt)
            except:
                continue

        if len(timestamps) < 2:
            return patterns

        # Analyze session frequency
        time_diffs = []
        for i in range(1, len(timestamps)):
            diff = (timestamps[i] - timestamps[i-1]).total_seconds() / 3600  # hours
            time_diffs.append(diff)

        avg_session_interval = sum(time_diffs) / len(time_diffs) if time_diffs else 0

        # Detect high-frequency periods
        if avg_session_interval < 1:  # Less than 1 hour between sessions
            patterns["high_frequency_sessions"] = PatternInfo(
                pattern_type="temporal",
                description="High frequency of sessions detected",
                frequency=len(sessions),
                impact_score=0.7,
                trend_score=0.6,
                urgency_score=0.5,
                examples=[f"Average {avg_session_interval:.1f} hours between sessions"],
                metadata={"avg_interval_hours": avg_session_interval}
            )

        # Analyze day of week patterns
        day_counts = Counter([ts.strftime("%A") for ts in timestamps])
        most_common_day, day_count = day_counts.most_common(1)[0]

        if day_count >= 3:
            patterns["day_of_week_pattern"] = PatternInfo(
                pattern_type="temporal",
                description=f"Most sessions on {most_common_day}",
                frequency=day_count,
                impact_score=0.4,
                trend_score=0.3,
                urgency_score=0.2,
                examples=[f"{day_count} sessions on {most_common_day}"],
                metadata={"day": most_common_day, "count": day_count}
            )

        # Analyze hour of day patterns
        hour_counts = Counter([ts.hour for ts in timestamps])
        if hour_counts:
            most_common_hour, hour_count = hour_counts.most_common(1)[0]

            patterns["time_of_day_pattern"] = PatternInfo(
                pattern_type="temporal",
                description=f"Most sessions around {most_common_hour}:00",
                frequency=hour_count,
                impact_score=0.3,
                trend_score=0.2,
                urgency_score=0.1,
                examples=[f"{hour_count} sessions around {most_common_hour}:00"],
                metadata={"hour": most_common_hour, "count": hour_count}
            )

        return patterns

    def analyze_file_correlations(self, sessions: List[SessionData]) -> Dict[str, PatternInfo]:
        """Analyze which files are modified together"""
        patterns = {}

        # Track file co-occurrences
        file_pairs = Counter()
        all_files = []

        for session in sessions:
            files = session.changed_files
            all_files.extend(files)

            # Count pairs
            for i, file1 in enumerate(files):
                for file2 in files[i+1:]:
                    pair = tuple(sorted([file1, file2]))
                    file_pairs[pair] += 1

        # Find frequently co-occurring files
        threshold = self.config.get("analysis", "file_correlation_threshold", default=2)

        for (file1, file2), count in file_pairs.most_common(10):
            if count >= threshold:
                pattern_key = f"file_correlation_{len(patterns)}"
                patterns[pattern_key] = PatternInfo(
                    pattern_type="file_correlation",
                    description=f"Files often modified together: {file1} and {file2}",
                    frequency=count,
                    impact_score=0.6,
                    trend_score=0.5,
                    urgency_score=0.4,
                    examples=[f"Modified together {count} times"],
                    metadata={"file1": file1, "file2": file2}
                )

        # Analyze file type patterns
        file_extensions = Counter()
        for file in all_files:
            if '.' in file:
                ext = file.split('.')[-1]
                file_extensions[ext] += 1

        for ext, count in file_extensions.most_common(5):
            if count >= 5:
                pattern_key = f"file_type_{ext}"
                patterns[pattern_key] = PatternInfo(
                    pattern_type="file_type",
                    description=f"Frequent work with .{ext} files",
                    frequency=count,
                    impact_score=0.7,
                    trend_score=0.6,
                    urgency_score=0.5,
                    examples=[f"{count} .{ext} files modified"],
                    metadata={"extension": ext}
                )

        return patterns

    def analyze_problem_recurrence(self, sessions: List[SessionData]) -> Dict[str, PatternInfo]:
        """Analyze recurring problems"""
        patterns = {}

        # Collect all problems
        all_problems = []
        for session in sessions:
            all_problems.extend(session.problems_solved)

        if not all_problems:
            return patterns

        # Extract keywords from problems
        problem_keywords = Counter()
        for problem in all_problems:
            # Simple keyword extraction (lowercase words)
            words = re.findall(r'\b\w+\b', problem.lower())
            # Filter out common words
            meaningful_words = [w for w in words if len(w) > 3 and w not in {'with', 'from', 'that', 'this', 'have', 'been'}]
            problem_keywords.update(meaningful_words)

        # Detect recurring problem themes
        threshold = self.config.get("analysis", "problem_recurrence_threshold", default=2)

        for keyword, count in problem_keywords.most_common(10):
            if count >= threshold:
                pattern_key = f"recurring_problem_{keyword}"
                patterns[pattern_key] = PatternInfo(
                    pattern_type="problem_recurrence",
                    description=f"Recurring issue related to: {keyword}",
                    frequency=count,
                    impact_score=0.8,
                    trend_score=0.7,
                    urgency_score=0.8,
                    examples=[p for p in all_problems if keyword in p.lower()][:3],
                    metadata={"keyword": keyword}
                )

        return patterns

    def analyze_commit_patterns(self, commits: List[Dict]) -> Dict[str, PatternInfo]:
        """Analyze commit message patterns"""
        patterns = {}

        if not commits:
            return patterns

        # Analyze commit message prefixes/types
        commit_types = Counter()
        for commit in commits:
            message = commit.get("message", "")
            # Extract emoji or prefix
            emoji_match = re.match(r'^([\U0001F300-\U0001F9FF]|:\w+:)', message)
            if emoji_match:
                commit_types[emoji_match.group(1)] += 1
            else:
                # Try to extract type like "feat:", "fix:", etc.
                type_match = re.match(r'^(\w+):', message)
                if type_match:
                    commit_types[type_match.group(1)] += 1

        # Detect dominant commit types
        for commit_type, count in commit_types.most_common(5):
            if count >= 3:
                pattern_key = f"commit_type_{commit_type}"
                patterns[pattern_key] = PatternInfo(
                    pattern_type="commit_pattern",
                    description=f"Frequent commit type: {commit_type}",
                    frequency=count,
                    impact_score=0.5,
                    trend_score=0.4,
                    urgency_score=0.3,
                    examples=[c["message"] for c in commits if commit_type in c["message"]][:3],
                    metadata={"commit_type": commit_type}
                )

        # Analyze commit keywords
        all_messages = " ".join([c.get("message", "") for c in commits])
        keywords = re.findall(r'\b\w{4,}\b', all_messages.lower())
        keyword_counts = Counter(keywords)

        for keyword, count in keyword_counts.most_common(10):
            if count >= 5 and keyword not in {'with', 'from', 'that', 'this', 'have'}:
                pattern_key = f"commit_keyword_{keyword}"
                patterns[pattern_key] = PatternInfo(
                    pattern_type="commit_keyword",
                    description=f"Frequent commit keyword: {keyword}",
                    frequency=count,
                    impact_score=0.6,
                    trend_score=0.5,
                    urgency_score=0.4,
                    examples=[c["message"] for c in commits if keyword in c["message"].lower()][:3],
                    metadata={"keyword": keyword}
                )

        return patterns

    def analyze_agent_collaboration(self, sessions: List[SessionData]) -> Dict[str, PatternInfo]:
        """Analyze how agents collaborate"""
        patterns = {}

        if len(sessions) < 2:
            return patterns

        # Analyze agent transitions
        agent_transitions = []
        for i in range(1, len(sessions)):
            prev_agent = sessions[i-1].agent
            curr_agent = sessions[i].agent
            if prev_agent != curr_agent:
                agent_transitions.append((prev_agent, curr_agent))

        if agent_transitions:
            transition_counts = Counter(agent_transitions)
            most_common_transition, count = transition_counts.most_common(1)[0]

            patterns["agent_handoff_pattern"] = PatternInfo(
                pattern_type="agent_collaboration",
                description=f"Frequent handoff: {most_common_transition[0]} â†’ {most_common_transition[1]}",
                frequency=count,
                impact_score=0.5,
                trend_score=0.4,
                urgency_score=0.3,
                examples=[f"Handoff occurred {count} times"],
                metadata={"from": most_common_transition[0], "to": most_common_transition[1]}
            )

        # Analyze agent specializations
        agent_file_types = defaultdict(Counter)
        for session in sessions:
            agent = session.agent
            for file in session.changed_files:
                if '.' in file:
                    ext = file.split('.')[-1]
                    agent_file_types[agent][ext] += 1

        for agent, extensions in agent_file_types.items():
            if extensions:
                top_ext, count = extensions.most_common(1)[0]
                patterns[f"agent_specialization_{agent}"] = PatternInfo(
                    pattern_type="agent_specialization",
                    description=f"{agent} frequently works with .{top_ext} files",
                    frequency=count,
                    impact_score=0.4,
                    trend_score=0.3,
                    urgency_score=0.2,
                    examples=[f"{count} .{top_ext} files"],
                    metadata={"agent": agent, "extension": top_ext}
                )

        return patterns

    def analyze_skill_gaps(self, sessions: List[SessionData], commits: List[Dict]) -> Dict[str, PatternInfo]:
        """Detect potential skill gaps"""
        patterns = {}

        # Analyze recurring problems that might need skills
        all_problems = []
        for session in sessions:
            all_problems.extend(session.problems_solved)

        # Detect domains that might need skills
        domain_keywords = {
            "testing": ["test", "testing", "unittest", "pytest", "jest"],
            "deployment": ["deploy", "docker", "kubernetes", "ci/cd", "pipeline"],
            "documentation": ["readme", "docs", "documentation", "markdown"],
            "api": ["api", "endpoint", "request", "response", "rest"],
            "database": ["database", "sql", "query", "migration"],
            "frontend": ["react", "vue", "angular", "css", "html"],
            "backend": ["server", "backend", "node", "python", "java"],
            "performance": ["performance", "optimization", "cache", "speed"],
            "security": ["security", "auth", "authentication", "permission"]
        }

        # Check which domains appear in problems and commits
        domain_scores = defaultdict(int)

        all_text = " ".join(all_problems + [c.get("message", "") for c in commits]).lower()

        for domain, keywords in domain_keywords.items():
            for keyword in keywords:
                if keyword in all_text:
                    domain_scores[domain] += all_text.count(keyword)

        # Detect skill gaps for high-frequency domains
        for domain, score in domain_scores.items():
            if score >= 5:
                patterns[f"skill_gap_{domain}"] = PatternInfo(
                    pattern_type="skill_gap",
                    description=f"Potential skill gap in {domain}",
                    frequency=score,
                    impact_score=0.8,
                    trend_score=0.7,
                    urgency_score=0.7,
                    examples=[f"{domain} mentioned {score} times"],
                    metadata={"domain": domain}
                )

        return patterns


# ============================================================================
# SKILL RECOMMENDER
# ============================================================================

class SkillRecommender:
    """Recommends skills based on detected patterns"""

    # Skill templates for different domains
    SKILL_TEMPLATES = {
        "testing": {
            "name": "TEST-GUARDIAN",
            "type": "testing",
            "description": "Automated testing assistance and test generation",
            "capabilities": ["test generation", "test automation", "coverage analysis"]
        },
        "deployment": {
            "name": "DEPLOY-SAGE",
            "type": "deployment",
            "description": "Deployment automation and CI/CD optimization",
            "capabilities": ["deployment automation", "CI/CD", "container management"]
        },
        "documentation": {
            "name": "DOC-GENIUS",
            "type": "documentation",
            "description": "Automatic documentation generation and maintenance",
            "capabilities": ["doc generation", "readme creation", "API documentation"]
        },
        "api": {
            "name": "API-MASTER",
            "type": "api",
            "description": "API design, implementation, and testing assistance",
            "capabilities": ["API design", "endpoint creation", "API testing"]
        },
        "performance": {
            "name": "PERF-OPTIMIZER",
            "type": "performance",
            "description": "Performance analysis and optimization",
            "capabilities": ["profiling", "optimization", "caching strategies"]
        },
        "security": {
            "name": "SECURITY-SHIELD",
            "type": "security",
            "description": "Security analysis and vulnerability detection",
            "capabilities": ["security scanning", "vulnerability detection", "auth implementation"]
        },
        "refactoring": {
            "name": "CODE-REFINER",
            "type": "refactoring",
            "description": "Code refactoring and quality improvement",
            "capabilities": ["code refactoring", "quality analysis", "pattern application"]
        },
        "data_processing": {
            "name": "DATA-WIZARD",
            "type": "data",
            "description": "Data processing and analysis automation",
            "capabilities": ["data transformation", "ETL", "analysis"]
        }
    }

    def __init__(self, config: ConfigManager):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def recommend_skills(self, patterns: Dict[str, PatternInfo]) -> List[SkillRecommendation]:
        """Generate skill recommendations based on patterns"""
        recommendations = []

        # Group patterns by domain
        domain_patterns = self._group_patterns_by_domain(patterns)

        # Generate recommendations for each domain
        for domain, domain_pattern_list in domain_patterns.items():
            if len(domain_pattern_list) >= 2 or any(p.frequency >= 5 for p in domain_pattern_list):
                recommendation = self._create_skill_recommendation(domain, domain_pattern_list)
                if recommendation:
                    recommendations.append(recommendation)

        # Sort by priority score
        recommendations.sort(key=lambda r: r.priority_score, reverse=True)

        # Filter by minimum score threshold
        min_score = self.config.get("thresholds", "recommendation_min_score", default=0.5)
        recommendations = [r for r in recommendations if r.priority_score >= min_score]

        self.logger.info(f"Generated {len(recommendations)} skill recommendations")

        return recommendations

    def _group_patterns_by_domain(self, patterns: Dict[str, PatternInfo]) -> Dict[str, List[PatternInfo]]:
        """Group patterns by domain/category"""
        domain_patterns = defaultdict(list)

        # Domain keywords mapping
        domain_keywords = {
            "testing": ["test", "testing", "unittest", "pytest"],
            "deployment": ["deploy", "docker", "ci/cd", "pipeline"],
            "documentation": ["readme", "docs", "documentation", "md"],
            "api": ["api", "endpoint", "request", "response"],
            "performance": ["performance", "optimization", "cache"],
            "security": ["security", "auth", "authentication"],
            "refactoring": ["refactor", "cleanup", "quality"],
            "data_processing": ["data", "csv", "json", "process"]
        }

        for pattern_key, pattern in patterns.items():
            # Check pattern type and description for domain keywords
            pattern_text = f"{pattern.pattern_type} {pattern.description}".lower()

            matched_domain = None
            for domain, keywords in domain_keywords.items():
                if any(keyword in pattern_text for keyword in keywords):
                    matched_domain = domain
                    break

            # Also check metadata for specific domains
            if not matched_domain:
                if "skill_gap" in pattern_key:
                    domain = pattern.metadata.get("domain")
                    if domain:
                        matched_domain = domain

            if matched_domain:
                domain_patterns[matched_domain].append(pattern)
            else:
                # General category
                domain_patterns["general"].append(pattern)

        return domain_patterns

    def _create_skill_recommendation(self, domain: str, patterns: List[PatternInfo]) -> Optional[SkillRecommendation]:
        """Create a skill recommendation for a domain"""

        # Get skill template
        template = self.SKILL_TEMPLATES.get(domain)
        if not template:
            return None

        # Calculate scores
        frequency_score = self._calculate_frequency_score(patterns)
        impact_score = self._calculate_impact_score(patterns)
        trend_score = self._calculate_trend_score(patterns)
        urgency_score = self._calculate_urgency_score(patterns)
        roi_score = self._calculate_roi_score(patterns)

        # Calculate weighted priority score
        weights = self.config.config["scoring"]
        priority_score = (
            frequency_score * weights["frequency_weight"] +
            impact_score * weights["impact_weight"] +
            trend_score * weights["trend_weight"] +
            urgency_score * weights["urgency_weight"] +
            roi_score * weights["roi_weight"]
        )

        # Generate reason
        reason = self._generate_recommendation_reason(domain, patterns)

        # Extract example use cases
        example_use_cases = []
        for pattern in patterns[:3]:
            example_use_cases.extend(pattern.examples[:2])

        return SkillRecommendation(
            skill_name=template["name"],
            skill_type=template["type"],
            description=template["description"],
            reason=reason,
            priority_score=priority_score,
            frequency_score=frequency_score,
            impact_score=impact_score,
            trend_score=trend_score,
            urgency_score=urgency_score,
            roi_score=roi_score,
            supporting_patterns=[p.description for p in patterns],
            example_use_cases=example_use_cases
        )

    def _calculate_frequency_score(self, patterns: List[PatternInfo]) -> float:
        """Calculate frequency score (0-1)"""
        total_frequency = sum(p.frequency for p in patterns)
        max_frequency = self.config.get("thresholds", "pattern_frequency_high", default=6) * len(patterns)
        return min(total_frequency / max_frequency, 1.0)

    def _calculate_impact_score(self, patterns: List[PatternInfo]) -> float:
        """Calculate impact score (0-1)"""
        if not patterns:
            return 0.0
        avg_impact = sum(p.impact_score for p in patterns) / len(patterns)
        return avg_impact

    def _calculate_trend_score(self, patterns: List[PatternInfo]) -> float:
        """Calculate trend score (0-1)"""
        if not patterns:
            return 0.0
        avg_trend = sum(p.trend_score for p in patterns) / len(patterns)
        return avg_trend

    def _calculate_urgency_score(self, patterns: List[PatternInfo]) -> float:
        """Calculate urgency score (0-1)"""
        if not patterns:
            return 0.0
        avg_urgency = sum(p.urgency_score for p in patterns) / len(patterns)
        return avg_urgency

    def _calculate_roi_score(self, patterns: List[PatternInfo]) -> float:
        """Calculate ROI score (0-1)"""
        # ROI based on frequency and impact
        if not patterns:
            return 0.0
        total_frequency = sum(p.frequency for p in patterns)
        avg_impact = sum(p.impact_score for p in patterns) / len(patterns)

        # Higher frequency + higher impact = higher ROI
        roi = (total_frequency / 10) * avg_impact
        return min(roi, 1.0)

    def _generate_recommendation_reason(self, domain: str, patterns: List[PatternInfo]) -> str:
        """Generate human-readable reason for recommendation"""
        pattern_count = len(patterns)
        total_frequency = sum(p.frequency for p in patterns)
        avg_impact = sum(p.impact_score for p in patterns) / len(patterns)

        reason = f"Detected {pattern_count} patterns in {domain} domain with {total_frequency} total occurrences. "

        if avg_impact > 0.7:
            reason += f"High impact ({avg_impact:.1f}) suggests significant productivity gains. "

        # Add specific pattern insights
        problem_patterns = [p for p in patterns if p.pattern_type == "problem_recurrence"]
        if problem_patterns:
            reason += f"Found {len(problem_patterns)} recurring problems that could be automated. "

        skill_gap_patterns = [p for p in patterns if p.pattern_type == "skill_gap"]
        if skill_gap_patterns:
            reason += f"Skill gap detected: frequent {domain} work without specialized tooling. "

        return reason.strip()


# ============================================================================
# REPORT GENERATOR
# ============================================================================

class ReportGenerator:
    """Generates detailed reports from analysis"""

    def __init__(self, config: ConfigManager):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def generate_report(
        self,
        patterns: Dict[str, PatternInfo],
        recommendations: List[SkillRecommendation],
        soul_data: Dict
    ) -> Dict:
        """Generate comprehensive report"""

        report = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "analyzer": "Synapse Pattern Detector",
                "version": "1.0.0"
            },
            "summary": self._generate_summary(patterns, recommendations, soul_data),
            "patterns": self._format_patterns(patterns),
            "recommendations": self._format_recommendations(recommendations),
            "trend_analysis": self._generate_trend_analysis(soul_data),
            "priority_matrix": self._generate_priority_matrix(recommendations),
            "actionable_insights": self._generate_insights(patterns, recommendations)
        }

        return report

    def _generate_summary(
        self,
        patterns: Dict[str, PatternInfo],
        recommendations: List[SkillRecommendation],
        soul_data: Dict
    ) -> Dict:
        """Generate executive summary"""
        sessions = soul_data.get("sessions", [])

        return {
            "total_sessions_analyzed": len(sessions),
            "patterns_detected": len(patterns),
            "skills_recommended": len(recommendations),
            "top_recommendation": recommendations[0].skill_name if recommendations else None,
            "highest_priority_score": recommendations[0].priority_score if recommendations else 0,
            "most_frequent_pattern": max(patterns.items(), key=lambda x: x[1].frequency)[0] if patterns else None
        }

    def _format_patterns(self, patterns: Dict[str, PatternInfo]) -> List[Dict]:
        """Format patterns for report"""
        formatted = []

        for pattern_key, pattern in sorted(patterns.items(), key=lambda x: x[1].frequency, reverse=True):
            formatted.append({
                "key": pattern_key,
                "type": pattern.pattern_type,
                "description": pattern.description,
                "frequency": pattern.frequency,
                "scores": {
                    "impact": pattern.impact_score,
                    "trend": pattern.trend_score,
                    "urgency": pattern.urgency_score
                },
                "examples": pattern.examples[:3],
                "metadata": pattern.metadata
            })

        return formatted

    def _format_recommendations(self, recommendations: List[SkillRecommendation]) -> List[Dict]:
        """Format recommendations for report"""
        formatted = []

        for rec in recommendations:
            formatted.append({
                "skill_name": rec.skill_name,
                "skill_type": rec.skill_type,
                "description": rec.description,
                "reason": rec.reason,
                "priority_score": round(rec.priority_score, 3),
                "detailed_scores": {
                    "frequency": round(rec.frequency_score, 3),
                    "impact": round(rec.impact_score, 3),
                    "trend": round(rec.trend_score, 3),
                    "urgency": round(rec.urgency_score, 3),
                    "roi": round(rec.roi_score, 3)
                },
                "supporting_patterns": rec.supporting_patterns,
                "example_use_cases": rec.example_use_cases[:5]
            })

        return formatted

    def _generate_trend_analysis(self, soul_data: Dict) -> Dict:
        """Generate trend analysis"""
        sessions = soul_data.get("sessions", [])

        if not sessions:
            return {}

        # Calculate averages
        total_files = sum(s.total_files for s in sessions)
        avg_files_per_session = total_files / len(sessions) if sessions else 0

        # Analyze session frequency
        timestamps = []
        for session in sessions:
            try:
                dt = datetime.strptime(session.timestamp, "%Y-%m-%d-%H:%M")
                timestamps.append(dt)
            except:
                continue

        session_frequency = 0
        if len(timestamps) >= 2:
            time_span = (timestamps[-1] - timestamps[0]).total_seconds() / 3600  # hours
            session_frequency = len(timestamps) / (time_span / 24) if time_span > 0 else 0  # sessions per day

        return {
            "session_frequency_per_day": round(session_frequency, 2),
            "avg_files_per_session": round(avg_files_per_session, 1),
            "total_sessions": len(sessions),
            "most_active_agent": max(sessions, key=lambda s: sessions.count(s)).agent if sessions else None
        }

    def _generate_priority_matrix(self, recommendations: List[SkillRecommendation]) -> Dict:
        """Generate priority matrix for recommendations"""

        high_priority = []
        medium_priority = []
        low_priority = []

        for rec in recommendations:
            if rec.priority_score >= 0.7:
                high_priority.append(rec.skill_name)
            elif rec.priority_score >= 0.5:
                medium_priority.append(rec.skill_name)
            else:
                low_priority.append(rec.skill_name)

        return {
            "high_priority": high_priority,
            "medium_priority": medium_priority,
            "low_priority": low_priority
        }

    def _generate_insights(
        self,
        patterns: Dict[str, PatternInfo],
        recommendations: List[SkillRecommendation]
    ) -> List[str]:
        """Generate actionable insights"""
        insights = []

        # Insight 1: Most urgent patterns
        urgent_patterns = [p for p in patterns.values() if p.urgency_score > 0.7]
        if urgent_patterns:
            insights.append(
                f"Found {len(urgent_patterns)} high-urgency patterns that should be addressed immediately"
            )

        # Insight 2: Top recommendation
        if recommendations:
            top_rec = recommendations[0]
            insights.append(
                f"Top recommendation: {top_rec.skill_name} with priority score {top_rec.priority_score:.2f}"
            )

        # Insight 3: Problem recurrence
        problem_patterns = [p for p in patterns.values() if p.pattern_type == "problem_recurrence"]
        if problem_patterns:
            insights.append(
                f"Detected {len(problem_patterns)} recurring problems that could be prevented with automation"
            )

        # Insight 4: Skill gaps
        skill_gap_patterns = [p for p in patterns.values() if p.pattern_type == "skill_gap"]
        if skill_gap_patterns:
            domains = [p.metadata.get("domain") for p in skill_gap_patterns if "domain" in p.metadata]
            if domains:
                insights.append(
                    f"Identified skill gaps in: {', '.join(set(domains))}"
                )

        # Insight 5: High ROI opportunities
        high_roi_recs = [r for r in recommendations if r.roi_score > 0.7]
        if high_roi_recs:
            insights.append(
                f"Found {len(high_roi_recs)} high-ROI skill opportunities that would provide significant value"
            )

        return insights

    def save_report(self, report: Dict, output_path: str, format: str = "both"):
        """Save report to file"""

        try:
            output_path_obj = Path(output_path)

            if format in ["json", "both"]:
                json_path = output_path_obj.with_suffix(".json")
                with open(json_path, 'w') as f:
                    json.dump(report, f, indent=2)
                self.logger.info(f"Saved JSON report to {json_path}")

            if format in ["text", "both"]:
                text_path = output_path_obj.with_suffix(".txt")
                text_report = self._format_text_report(report)
                with open(text_path, 'w') as f:
                    f.write(text_report)
                self.logger.info(f"Saved text report to {text_path}")

        except Exception as e:
            self.logger.error(f"Error saving report: {e}")

    def _format_text_report(self, report: Dict) -> str:
        """Format report as human-readable text"""
        lines = []

        lines.append("=" * 80)
        lines.append("Synapse PATTERN DETECTION REPORT")
        lines.append("=" * 80)
        lines.append("")

        # Metadata
        lines.append(f"Generated: {report['metadata']['generated_at']}")
        lines.append(f"Analyzer: {report['metadata']['analyzer']}")
        lines.append("")

        # Summary
        lines.append("SUMMARY")
        lines.append("-" * 80)
        summary = report['summary']
        for key, value in summary.items():
            lines.append(f"{key.replace('_', ' ').title()}: {value}")
        lines.append("")

        # Recommendations
        lines.append("SKILL RECOMMENDATIONS")
        lines.append("-" * 80)
        for i, rec in enumerate(report['recommendations'], 1):
            lines.append(f"\n{i}. {rec['skill_name']} (Priority: {rec['priority_score']:.2f})")
            lines.append(f"   Type: {rec['skill_type']}")
            lines.append(f"   Description: {rec['description']}")
            lines.append(f"   Reason: {rec['reason']}")
            lines.append(f"   Scores:")
            for score_name, score_value in rec['detailed_scores'].items():
                lines.append(f"     - {score_name.title()}: {score_value:.2f}")
        lines.append("")

        # Priority Matrix
        lines.append("PRIORITY MATRIX")
        lines.append("-" * 80)
        matrix = report['priority_matrix']
        lines.append(f"High Priority ({len(matrix['high_priority'])}): {', '.join(matrix['high_priority']) if matrix['high_priority'] else 'None'}")
        lines.append(f"Medium Priority ({len(matrix['medium_priority'])}): {', '.join(matrix['medium_priority']) if matrix['medium_priority'] else 'None'}")
        lines.append(f"Low Priority ({len(matrix['low_priority'])}): {', '.join(matrix['low_priority']) if matrix['low_priority'] else 'None'}")
        lines.append("")

        # Actionable Insights
        lines.append("ACTIONABLE INSIGHTS")
        lines.append("-" * 80)
        for i, insight in enumerate(report['actionable_insights'], 1):
            lines.append(f"{i}. {insight}")
        lines.append("")

        # Patterns Summary
        lines.append("DETECTED PATTERNS")
        lines.append("-" * 80)
        lines.append(f"Total patterns detected: {len(report['patterns'])}")
        lines.append("\nTop 10 patterns by frequency:")
        for i, pattern in enumerate(report['patterns'][:10], 1):
            lines.append(f"{i}. {pattern['description']} (Frequency: {pattern['frequency']})")
        lines.append("")

        lines.append("=" * 80)
        lines.append("End of Report")
        lines.append("=" * 80)

        return "\n".join(lines)


# ============================================================================
# MAIN PATTERN DETECTOR
# ============================================================================

class PatternDetectorMain:
    """Main pattern detector orchestrator"""

    def __init__(self, config_path: Optional[str] = None, repo_root: str = "."):
        self.config = ConfigManager(config_path)
        self.repo_root = repo_root

        # Setup logging
        log_level = logging.DEBUG if self.config.get("output", "verbose", default=True) else logging.INFO
        logging.basicConfig(
            level=log_level,
            format='[%(levelname)s] %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # Validate config
        if not self.config.validate():
            self.logger.warning("Configuration validation failed, using defaults")

        # Initialize components
        self.soul_reader = CortexDataReader(repo_root, self.config)
        self.pattern_detector = PatternDetector(self.config)
        self.skill_recommender = SkillRecommender(self.config)
        self.report_generator = ReportGenerator(self.config)

    def run_analysis(self, output_path: Optional[str] = None) -> Dict:
        """Run complete pattern analysis"""

        self.logger.info("Starting Synapse pattern detection analysis")

        # Step 1: Read Cortex data
        self.logger.info("Reading Cortex data...")
        soul_data = self.soul_reader.read_all_soul_data()

        if not soul_data.get("sessions"):
            self.logger.error("No session data found. Cannot perform analysis.")
            return {}

        # Step 2: Detect patterns
        self.logger.info("Detecting patterns...")
        patterns = self.pattern_detector.analyze_all_patterns(soul_data)

        if not patterns:
            self.logger.warning("No patterns detected")

        # Step 3: Generate recommendations
        self.logger.info("Generating skill recommendations...")
        recommendations = self.skill_recommender.recommend_skills(patterns)

        # Step 4: Generate report
        self.logger.info("Generating report...")
        report = self.report_generator.generate_report(patterns, recommendations, soul_data)

        # Step 5: Save report if output path specified
        if output_path:
            report_format = self.config.get("output", "report_format", default="both")
            self.report_generator.save_report(report, output_path, format=report_format)

        self.logger.info("Analysis complete!")

        return report

    def print_summary(self, report: Dict):
        """Print report summary to console"""

        print("\n" + "=" * 80)
        print("Synapse PATTERN DETECTION - SUMMARY")
        print("=" * 80)

        summary = report.get("summary", {})
        print(f"\nSessions Analyzed: {summary.get('total_sessions_analyzed', 0)}")
        print(f"Patterns Detected: {summary.get('patterns_detected', 0)}")
        print(f"Skills Recommended: {summary.get('skills_recommended', 0)}")

        if summary.get('top_recommendation'):
            print(f"\nTop Recommendation: {summary['top_recommendation']}")
            print(f"Priority Score: {summary.get('highest_priority_score', 0):.2f}")

        # Print top 3 recommendations
        recommendations = report.get("recommendations", [])
        if recommendations:
            print("\n" + "-" * 80)
            print("TOP SKILL RECOMMENDATIONS:")
            print("-" * 80)
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"\n{i}. {rec['skill_name']} (Priority: {rec['priority_score']:.2f})")
                print(f"   {rec['description']}")
                print(f"   Reason: {rec['reason'][:100]}...")

        # Print actionable insights
        insights = report.get("actionable_insights", [])
        if insights:
            print("\n" + "-" * 80)
            print("ACTIONABLE INSIGHTS:")
            print("-" * 80)
            for i, insight in enumerate(insights, 1):
                print(f"{i}. {insight}")

        print("\n" + "=" * 80)


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Synapse Pattern Detector - Analyze Cortex files and recommend skills",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze current repository
  python pattern_detector.py

  # Analyze with custom config
  python pattern_detector.py --config nexus_config.json

  # Specify repository root
  python pattern_detector.py --repo /path/to/repo

  # Save report to file
  python pattern_detector.py --output pattern_report

  # Generate JSON report only
  python pattern_detector.py --output report --format json
        """
    )

    parser.add_argument(
        "--config",
        help="Path to Synapse configuration file (JSON)",
        type=str
    )

    parser.add_argument(
        "--repo",
        help="Repository root path (default: current directory)",
        type=str,
        default="."
    )

    parser.add_argument(
        "--output",
        help="Output path for report (without extension)",
        type=str
    )

    parser.add_argument(
        "--format",
        help="Report format: json, text, or both (default: both)",
        choices=["json", "text", "both"],
        default="both"
    )

    parser.add_argument(
        "--quiet",
        help="Suppress console output",
        action="store_true"
    )

    parser.add_argument(
        "--version",
        help="Show version and exit",
        action="version",
        version="Synapse Pattern Detector v1.0.0"
    )

    args = parser.parse_args()

    # Initialize detector
    detector = PatternDetectorMain(
        config_path=args.config,
        repo_root=args.repo
    )

    # Run analysis
    report = detector.run_analysis(output_path=args.output)

    # Print summary unless quiet mode
    if not args.quiet and report:
        detector.print_summary(report)

    return 0 if report else 1


if __name__ == "__main__":
    exit(main())
