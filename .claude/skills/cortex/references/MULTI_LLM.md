# Cortex Multi-LLM Support

Cortex works with Claude Code, GPT, Gemini, and other CLI-based LLMs.

## Universal Format

Cortex uses LLM-agnostic formats:
- **Markdown** for human-readable logs
- **JSON** for machine-readable state
- **Python API** for programmatic access

No LLM-specific syntax or dependencies.

## Supported LLMs

### Claude Code (Primary)

**Installation:**
```bash
cd .claude/skills/cortex/scripts
./install.sh
```

**How It Works:**
- Claude reads `.cortex_handoff.md` at session start
- Git hooks automatically update Cortex files
- Skills use Cortex API for inter-skill communication

**Skill Loading:**
```bash
claude --skills soul,nexus
```

### OpenAI GPT (CLI)

**Installation:**

1. Install Cortex git hooks:
   ```bash
   cd .claude/skills/cortex/scripts
   ./install.sh
   ```

2. Create GPT config in `~/.config/gpt/config.yaml`:
   ```yaml
   system_prompt: |
     You have access to Cortex session memory.
     At the start of each session, read .cortex_handoff.md
     for context about previous work.

     Cortex files:
     - .cortex_handoff.md - Quick start for this session
     - .cortex_log.md - Detailed session history
     - .cortex_status.json - Current state

     Update Cortex at the end of significant work:
     python3 .claude/skills/cortex/scripts/trace_session.py
   ```

**Usage:**
```bash
# Start session
gpt

# At session start, GPT reads handoff
cat .cortex_handoff.md

# After completing work
python3 .claude/skills/cortex/scripts/trace_session.py
```

### Google Gemini (CLI)

**Installation:**

1. Install Cortex git hooks:
   ```bash
   cd .claude/skills/cortex/scripts
   ./install.sh
   ```

2. Add to your Gemini CLI config:
   ```json
   {
     "pre_session_commands": [
       "cat .cortex_handoff.md"
     ],
     "post_session_commands": [
       "python3 .claude/skills/cortex/scripts/trace_session.py"
     ]
   }
   ```

**Manual Usage:**
```bash
# Start session
gemini

# Ask Gemini to read handoff
> Read .cortex_handoff.md and tell me where we left off

# After work, update Cortex
> Run trace_session.py to update Cortex
```

### Cursor / Copilot

**Installation:**

1. Install git hooks:
   ```bash
   cd .claude/skills/cortex/scripts
   ./install.sh
   ```

2. Git hooks will automatically track your work

**Manual Handoff:**
```bash
# Before context switch
python3 .claude/skills/cortex/scripts/handoff_generator.py

# Start new session, read handoff
cat .cortex_handoff.md
```

### Aider

**Installation:**

1. Install Cortex:
   ```bash
   cd .claude/skills/cortex/scripts
   ./install.sh
   ```

2. Add to `.aider.conf.yml`:
   ```yaml
   auto-commits: true
   read:
     - .cortex_handoff.md

   message: |
     You have access to Cortex memory system.
     Check .cortex_handoff.md for session context.
   ```

**Usage:**

Aider will automatically:
- Read `.cortex_handoff.md` at session start
- Trigger git hooks on commits (updating Cortex)
- Maintain session continuity

### Generic LLM CLI

For any LLM with file access:

1. **Install git hooks:**
   ```bash
   cd .claude/skills/cortex/scripts
   ./install.sh
   ```

2. **At session start**, ask LLM to:
   ```
   Read .cortex_handoff.md and summarize what was done previously
   ```

3. **During session**, LLM can:
   ```
   Read .cortex_log.md for detailed history
   Read .cortex_status.json for current state
   ```

4. **At session end**, update Cortex:
   ```bash
   python3 .claude/skills/cortex/scripts/trace_session.py
   ```

---

## LLM-Specific Instructions

### For Claude Code

No special instructions needed - Cortex works natively.

### For GPT

Add this to your prompts:

```
Before starting, check .cortex_handoff.md for session context.

After completing work:
- Update git
- Run: python3 .claude/skills/cortex/scripts/trace_session.py
```

### For Gemini

Add this to your prompts:

```
Session memory is in .cortex_handoff.md. Read it first.

When making commits, Cortex will automatically update via git hooks.

To manually update Cortex:
python3 .claude/skills/cortex/scripts/trace_session.py
```

### For Cursor

Add to workspace settings (`.vscode/settings.json`):

```json
{
  "cursor.ai.systemPrompt": "Check .cortex_handoff.md at session start for previous context. Cortex tracks sessions automatically via git hooks."
}
```

---

## Python API Usage (All LLMs)

All LLMs can use Cortex's Python API:

```python
import sys
from pathlib import Path

# Add Cortex to path
soul_scripts = Path(".claude/skills/cortex/scripts")
sys.path.insert(0, str(soul_scripts))

from cortex_api import add_cortex_event, get_cortex_memory

# Record events
add_cortex_event("api_call", "Called external API")

# Query memory
events = get_cortex_memory(filter_type="api_call")
```

No LLM-specific code required.

---

## File Compatibility

Cortex files use universal formats:

### `.cortex_handoff.md`

Plain markdown - readable by any LLM:

```markdown
# Agent Handoff - 2025-10-26

## Status
- 5 files changed
- Tests passing
- On branch: feature/new-api

## Recent Work
- Implemented user authentication
- Added JWT token support

## Next Steps
1. Add refresh token logic
2. Write integration tests
```

### `.cortex_log.md`

Timestamped markdown log:

```markdown
# Agent Session Log

## 2025-10-26 09:30

**Task**: Implement authentication
**Files Modified**: auth.py, tokens.py
**Outcome**: Completed - tests passing
```

### `.cortex_status.json`

Standard JSON - parseable by any language:

```json
{
  "current_branch": "feature/new-api",
  "files_changed": 5,
  "last_commit": "feat: Add authentication",
  "events": [
    {
      "timestamp": "2025-10-26T09:30:15",
      "event_type": "api_call",
      "description": "Called auth service"
    }
  ]
}
```

---

## Cross-LLM Workflows

### Scenario: GPT starts, Claude finishes

**Developer using GPT:**
```bash
# Work with GPT
gpt
> Implement new feature X

# GPT commits work, git hooks update Cortex
git commit -m "WIP: Feature X partially done"

# Cortex files are automatically updated
```

**Developer using Claude:**
```bash
# Switch to Claude
claude

# Claude automatically reads .cortex_handoff.md
# Sees what GPT did, continues seamlessly
```

Cortex enables cross-LLM handoffs!

---

## Team Collaboration

Different team members can use different LLMs:

```
Developer A (Claude) → commits with Cortex → pushes
Developer B (GPT) → pulls, reads Cortex → continues
Developer C (Gemini) → pulls, reads Cortex → continues
```

Cortex provides universal context format.

---

## Limitations by LLM

| Feature | Claude Code | GPT CLI | Gemini CLI | Cursor | Aider |
|---------|-------------|---------|------------|--------|-------|
| Auto handoff read | ✅ Native | ⚠️ Manual | ⚠️ Manual | ⚠️ Manual | ✅ Config |
| Git hook auto-update | ✅ | ✅ | ✅ | ✅ | ✅ |
| Python API access | ✅ | ✅ | ✅ | ⚠️ Limited | ✅ |
| Skill-to-skill communication | ✅ Full | ⚠️ Manual | ⚠️ Manual | ❌ No | ⚠️ Limited |
| Synapse integration | ✅ Full | ⚠️ Partial | ⚠️ Partial | ❌ No | ⚠️ Partial |

**Legend:**
- ✅ Full support
- ⚠️ Works with manual steps
- ❌ Not supported

---

## Best Practices for Multi-LLM

1. **Always commit Cortex files** - Enables cross-LLM handoffs
2. **Standardize event types** - Use consistent naming across LLMs
3. **Use Python API when possible** - More reliable than manual updates
4. **Read handoff at session start** - Even if LLM doesn't do it automatically
5. **Update Cortex at session end** - `trace_session.py` before context switch

---

## Troubleshooting

### LLM doesn't see Cortex files

**Solution:** Explicitly ask LLM to read them:
```
Read .cortex_handoff.md and tell me what was done previously
```

### Git hooks not triggering

**Solution:** Reinstall hooks:
```bash
cd .claude/skills/cortex/scripts
./install.sh
```

Check `.git/hooks/post-commit` exists and is executable.

### Python API import errors

**Solution:** Add Cortex to Python path:
```python
import sys
from pathlib import Path
sys.path.insert(0, str(Path(".claude/skills/cortex/scripts")))
```

### Cortex files conflict on merge

**Solution:** Cortex files are auto-generated, keep latest:
```bash
git checkout --theirs .cortex_log.md
git checkout --theirs .cortex_status.json
git checkout --theirs .cortex_handoff.md
```

Or regenerate after merge:
```bash
python3 .claude/skills/cortex/scripts/trace_session.py
```

---

## Extending Cortex for Custom LLMs

To add support for a new LLM:

1. **Install git hooks** - Universal across all LLMs
2. **Configure LLM** - Add Cortex file reading to system prompt
3. **Add to documentation** - Submit PR to add your LLM config

Example contribution:

```markdown
### MyNewLLM

**Installation:**
1. Install git hooks: `cd .claude/skills/cortex/scripts && ./install.sh`
2. Add to `~/.mynewllm/config`:
   ```
   pre_session: cat .cortex_handoff.md
   ```

**Usage:**
```bash
mynewllm
# Automatically reads Cortex handoff
```
```

---

## See Also

- [API_REFERENCE.md](API_REFERENCE.md) - Complete API documentation
- [WORKFLOWS.md](WORKFLOWS.md) - Common usage patterns
- Main SKILL.md - Cortex overview
